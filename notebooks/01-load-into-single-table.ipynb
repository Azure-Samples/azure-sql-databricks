{"cells":[{"cell_type":"markdown","source":["# 01 - Load data into an Azure SQL heap, non-partitioned, non-indexed, table\n\nIn Azure SQL terminology an Heap is a table with no clustered index. In this samples we'll load data into a table that as no index (clustered or non-clustered) as is not partitioned. This is the simplest scenario possibile and allows parallel load of data.\n\nSample is using both the new sql-spark-connector (https://github.com/microsoft/sql-spark-connector), and the previous one (https://github.com/Azure/azure-sqldb-spark). To install the _new connector_ manually import the .jar file (available in GitHub repo's releases) into the cluster. To install the previous one, just import the library right from Databricks portal using the \"com.microsoft.azure:azure-sqldb-spark:1.0.2\" coordinates."],"metadata":{}},{"cell_type":"markdown","source":["Define variables used thoughout the script. Azure Key Value has been used to securely store sensitive data. More info here: [Create an Azure Key Vault-backed secret scope](https://docs.microsoft.com/en-us/azure/databricks/security/secrets/secret-scopes#--create-an-azure-key-vault-backed-secret-scope)"],"metadata":{}},{"cell_type":"code","source":["val scope = \"key-vault-secrets\"\n\nval storageAccount = \"dmstore2\";\nval storageKey = dbutils.secrets.get(scope, \"dmstore2-2\");\n\nval server = dbutils.secrets.get(scope, \"srv001\").concat(\".database.windows.net\");\nval database = dbutils.secrets.get(scope, \"db001\");\nval user = dbutils.secrets.get(scope, \"dbuser001\");\nval password = dbutils.secrets.get(scope, \"dbpwd001\");\nval table = \"dbo.LINEITEM_LOADTEST\"\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">scope: String = key-vault-secrets\nstorageAccount: String = dmstore2\nstorageKey: String = [REDACTED]\nserver: String = [REDACTED].database.windows.net\ndatabase: String = [REDACTED]\nuser: String = [REDACTED]\npassword: String = [REDACTED]\ntable: String = dbo.LINEITEM_LOADTEST\n</div>"]}}],"execution_count":3},{"cell_type":"markdown","source":["Configure Spark to access Azure Blob Store"],"metadata":{}},{"cell_type":"code","source":["spark.conf.set(s\"fs.azure.account.key.$storageAccount.blob.core.windows.net\", storageKey);"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["Load the Parquet file generated in `00-create-parquet-file` notebook that contains LINEITEM data partitioned by Year and Month"],"metadata":{}},{"cell_type":"code","source":["val li = spark.read.parquet(\"wasbs://tpch@dmstore2.blob.core.windows.net/10GB/parquet/lineitem\")"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["Loaded data is split in 20 dataframe partitions"],"metadata":{}},{"cell_type":"code","source":["li.rdd.getNumPartitions"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res2: Int = 20\n</div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["Show schema of loaded data"],"metadata":{}},{"cell_type":"code","source":["li.printSchema"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- L_ORDERKEY: integer (nullable = true)\n-- L_PARTKEY: integer (nullable = true)\n-- L_SUPPKEY: integer (nullable = true)\n-- L_LINENUMBER: integer (nullable = true)\n-- L_QUANTITY: decimal(15,2) (nullable = true)\n-- L_EXTENDEDPRICE: decimal(15,2) (nullable = true)\n-- L_DISCOUNT: decimal(15,2) (nullable = true)\n-- L_TAX: decimal(15,2) (nullable = true)\n-- L_RETURNFLAG: string (nullable = true)\n-- L_LINESTATUS: string (nullable = true)\n-- L_SHIPDATE: date (nullable = true)\n-- L_COMMITDATE: date (nullable = true)\n-- L_RECEIPTDATE: date (nullable = true)\n-- L_SHIPINSTRUCT: string (nullable = true)\n-- L_SHIPMODE: string (nullable = true)\n-- L_COMMENT: string (nullable = true)\n-- L_PARTITION_KEY: integer (nullable = true)\n\n</div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["All columns are shown as nullable, even if they were originally set to NOT NULL, so we will need to keep this in mind later."],"metadata":{}},{"cell_type":"markdown","source":["Make sure you create on your Azure SQL the following LINEITEM table:\n```sql\ncreate table [dbo].[LINEITEM_LOADTEST]\n(\n\t[L_ORDERKEY] [int] not null,\n\t[L_PARTKEY] [int] not null,\n\t[L_SUPPKEY] [int] not null,\n\t[L_LINENUMBER] [int] not null,\n\t[L_QUANTITY] [decimal](15, 2) not null,\n\t[L_EXTENDEDPRICE] [decimal](15, 2) not null,\n\t[L_DISCOUNT] [decimal](15, 2) not null,\n\t[L_TAX] [decimal](15, 2) not null,\n\t[L_RETURNFLAG] [char](1) not null,\n\t[L_LINESTATUS] [char](1) not null,\n\t[L_SHIPDATE] [date] not null,\n\t[L_COMMITDATE] [date] not null,\n\t[L_RECEIPTDATE] [date] not null,\n\t[L_SHIPINSTRUCT] [char](25) not null,\n\t[L_SHIPMODE] [char](10) not null,\n\t[L_COMMENT] [varchar](44) not null,\n\t[L_PARTITION_KEY] [int] not null\n) \n```"],"metadata":{}},{"cell_type":"markdown","source":["## Using the new connector"],"metadata":{}},{"cell_type":"markdown","source":["Schema needs to be defined explicitly as new connector is very sensitive to nullability, as per the following issue [Nullable column mismatch between Spark DataFrame & SQL Table Error](\nhttps://github.com/microsoft/sql-spark-connector/issues/5), so we need to explicity create the schema and apply it to the loaded data"],"metadata":{}},{"cell_type":"code","source":["import org.apache.spark.sql.types._\n\nval schema = StructType(\n    StructField(\"L_ORDERKEY\", IntegerType, false) ::\n    StructField(\"L_PARTKEY\", IntegerType, false) ::\n    StructField(\"L_SUPPKEY\", IntegerType, false) ::  \n    StructField(\"L_LINENUMBER\", IntegerType, false) ::\n    StructField(\"L_QUANTITY\", DecimalType(15,2), false) ::\n    StructField(\"L_EXTENDEDPRICE\", DecimalType(15,2), false) ::\n    StructField(\"L_DISCOUNT\", DecimalType(15,2), false) ::\n    StructField(\"L_TAX\", DecimalType(15,2), false) ::\n    StructField(\"L_RETURNFLAG\", StringType, false) ::\n    StructField(\"L_LINESTATUS\", StringType, false) ::\n    StructField(\"L_SHIPDATE\", DateType, false) ::\n    StructField(\"L_COMMITDATE\", DateType, false) ::\n    StructField(\"L_RECEIPTDATE\", DateType, false) ::\n    StructField(\"L_SHIPINSTRUCT\", StringType, false) ::  \n    StructField(\"L_SHIPMODE\", StringType, false) ::  \n    StructField(\"L_COMMENT\", StringType, false) ::  \n    StructField(\"L_PARTITION_KEY\", IntegerType, false) ::  \n    Nil)\n    \n  val li2 = spark.createDataFrame(li.rdd, schema)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">import org.apache.spark.sql.types._\nschema: org.apache.spark.sql.types.StructType = StructType(StructField(L_ORDERKEY,IntegerType,false), StructField(L_PARTKEY,IntegerType,false), StructField(L_SUPPKEY,IntegerType,false), StructField(L_LINENUMBER,IntegerType,false), StructField(L_QUANTITY,DecimalType(15,2),false), StructField(L_EXTENDEDPRICE,DecimalType(15,2),false), StructField(L_DISCOUNT,DecimalType(15,2),false), StructField(L_TAX,DecimalType(15,2),false), StructField(L_RETURNFLAG,StringType,false), StructField(L_LINESTATUS,StringType,false), StructField(L_SHIPDATE,DateType,false), StructField(L_COMMITDATE,DateType,false), StructField(L_RECEIPTDATE,DateType,false), StructField(L_SHIPINSTRUCT,StringType,false), StructField(L_SHIPMODE,StringType,false), StructField(L_COMMENT,StringType,false), StructField(L_PARTITION_KEY,IntegerType,false))\nli2: org.apache.spark.sql.DataFrame = [L_ORDERKEY: int, L_PARTKEY: int ... 15 more fields]\n</div>"]}}],"execution_count":16},{"cell_type":"code","source":["val url = s\"jdbc:sqlserver://$server;databaseName=$database;\"\n\nli2.write \n  .format(\"com.microsoft.sqlserver.jdbc.spark\") \n  .mode(\"overwrite\")   \n  .option(\"truncate\", \"true\") \n  .option(\"url\", url) \n  .option(\"dbtable\", table) \n  .option(\"user\", user) \n  .option(\"password\", password) \n  .option(\"reliabilityLevel\", \"BEST_EFFORT\") \n  .option(\"tableLock\", \"true\") \n  .option(\"batchsize\", \"100000\") \n  .save()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">url: String = jdbc:sqlserver://[REDACTED].database.windows.net;databaseName=[REDACTED];\n</div>"]}}],"execution_count":17},{"cell_type":"markdown","source":["# Using the old connector:"],"metadata":{}},{"cell_type":"markdown","source":["This connector is more permissive about schema so we can just use the schema coming from Parquet file"],"metadata":{}},{"cell_type":"code","source":["import com.microsoft.azure.sqldb.spark.bulkcopy.BulkCopyMetadata\nimport com.microsoft.azure.sqldb.spark.config.Config\nimport com.microsoft.azure.sqldb.spark.connect._\n\nval config = Config(Map(\n  \"url\" -> server,\n  \"databaseName\" -> database,\n  \"dbTable\" -> table,\n  \"user\" -> user,\n  \"password\" -> password,\n  \"bulkCopyBatchSize\" -> \"100000\",\n  \"bulkCopyTableLock\" -> \"true\",  \n  \"bulkCopyTimeout\" -> \"600\" //seconds  \n))\n\nli.bulkCopyToSqlDB(config)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">import com.microsoft.azure.sqldb.spark.bulkcopy.BulkCopyMetadata\nimport com.microsoft.azure.sqldb.spark.config.Config\nimport com.microsoft.azure.sqldb.spark.connect._\nconfig: com.microsoft.azure.sqldb.spark.config.Config = com.microsoft.azure.sqldb.spark.config.ConfigBuilder$$anon$1@b4be6705\n</div>"]}}],"execution_count":20}],"metadata":{"name":"01-load-into-single-table","notebookId":3644482941518162},"nbformat":4,"nbformat_minor":0}