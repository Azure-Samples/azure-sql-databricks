{"cells":[{"cell_type":"code","source":["// https://medium.com/@mrpowers/managing-spark-partitions-with-coalesce-and-repartition-4050c57ad5c4\n// from https://docs.databricks.com/notebooks/notebook-workflows.html#api\n\nimport scala.concurrent.{Future, Await}\nimport scala.concurrent.duration._\nimport scala.util.control.NonFatal\n\ncase class NotebookData(path: String, timeout: Int, parameters: Map[String, String] = Map.empty[String, String])\n\ndef parallelNotebooks(notebooks: Seq[NotebookData]): Future[Seq[String]] = {\n  import scala.concurrent.{Future, blocking, Await}\n  import java.util.concurrent.Executors\n  import scala.concurrent.ExecutionContext\n  import com.databricks.WorkflowException\n\n  val numNotebooksInParallel = 4 \n  // If you create too many notebooks in parallel the driver may crash when you submit all of the jobs at once. \n  // This code limits the number of parallel notebooks.\n  implicit val ec = ExecutionContext.fromExecutor(Executors.newFixedThreadPool(numNotebooksInParallel))\n  val ctx = dbutils.notebook.getContext()\n  \n  Future.sequence(\n    notebooks.map { notebook => \n      Future {\n        dbutils.notebook.setContext(ctx)\n        if (notebook.parameters.nonEmpty)\n          dbutils.notebook.run(notebook.path, notebook.timeout, notebook.parameters)\n        else\n          dbutils.notebook.run(notebook.path, notebook.timeout)\n      }\n      .recover {\n        case NonFatal(e) => s\"ERROR: ${e.getMessage}\"\n      }\n    }\n  )\n}"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">import scala.concurrent.{Future, Await}\nimport scala.concurrent.duration._\nimport scala.util.control.NonFatal\ndefined class NotebookData\nparallelNotebooks: (notebooks: Seq[NotebookData])scala.concurrent.Future[Seq[String]]\n</div>"]}}],"execution_count":1},{"cell_type":"code","source":["import spark.implicits._\nimport org.apache.spark.sql._\n\ncase class partitionToProcess(partitionKey:Int)\n\nval ptp = Seq(\n    partitionToProcess(199702),\n    partitionToProcess(199703),\n    partitionToProcess(199705)\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">import spark.implicits._\nimport org.apache.spark.sql._\ndefined class partitionToProcess\nptp: Seq[partitionToProcess] = List(partitionToProcess(199702), partitionToProcess(199703), partitionToProcess(199705))\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["import scala.concurrent.Await\nimport scala.concurrent.duration._\nimport scala.language.postfixOps\n\nval notebooks = ptp.map(p => NotebookData(\"./partition-load-one\", 180, Map(\"partitionKey\" -> p.partitionKey.toString)))\n\nval res = parallelNotebooks(notebooks)\n\nAwait.result(res, 180 seconds) // this is a blocking call.\n\nres.value"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">import scala.concurrent.Await\nimport scala.concurrent.duration._\nimport scala.language.postfixOps\nnotebooks: Seq[NotebookData] = List(NotebookData(./partition-load-one,180,Map(partitionKey -&gt; 199702)), NotebookData(./partition-load-one,180,Map(partitionKey -&gt; 199703)), NotebookData(./partition-load-one,180,Map(partitionKey -&gt; 199705)))\nres: scala.concurrent.Future[Seq[String]] = Future(Success(List(199702, 199703, 199705)))\nres0: Option[scala.util.Try[Seq[String]]] = Some(Success(List(199702, 199703, 199705)))\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":4}],"metadata":{"name":"03a-parallel-load-into-partitioned-table-many","notebookId":964636935775876},"nbformat":4,"nbformat_minor":0}