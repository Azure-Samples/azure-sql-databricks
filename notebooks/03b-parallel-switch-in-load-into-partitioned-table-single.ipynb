{"cells":[{"cell_type":"markdown","source":["# 03b - Parallel Switch-In Load Into Partitioned Table - Sigle Partition Load\n\nThis notebook will bulk load data into exactly one Azure SQL partition. It accepts a Partition Key as a parameter, and that value will be used to load all data that belongs to that partition. In this sample column used to partition data is the `L_PARTITION_KEY` column, which is an integer, so the provided partition key *must be* an integer too.\n\nData is not loaded directly into the selected partition, but a staging table is created, loaded and then switched into the target table, becoming the desired partition.\n\nMore info on this switch-in technique can be found in the related notebook: `03a-parallel-switch-in-load-into-partitioned-table-many`\n\n## Notes on terminology\n\nThe term \"row-store\" is used to identify and index that is not using the [column-store layout](https://docs.microsoft.com/en-us/sql/relational-databases/indexes/columnstore-indexes-overview) to store its data.\n\n## Sample\n\nThis notebook is used to load exactly on partition of a partitioned table by loading a staging table and then switching it in into the target table. The process is the following:\n\n- Create a staging table\n- Load staging table\n- Create indexes\n- Create check constraints\n- Execute switch-in operation\n\nMore details on this pattern can be found in [this post](https://www.cathrinewilhelmsen.net/2015/04/19/table-partitioning-in-sql-server-partition-switching/) written by the Data Platform MVP Cathrine Wilhelmsen. \n)\n\n## Supported Azure Databricks Versions\n\nDatabricks supported versions: Spark 2.4.5 and Scala 2.11"],"metadata":{}},{"cell_type":"markdown","source":["## Setup"],"metadata":{}},{"cell_type":"markdown","source":["Define notebook parameter:"],"metadata":{}},{"cell_type":"code","source":["dbutils.widgets.text(\"partitionKey\", \"0\", \"Partition Key\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["Define variables used thoughout the script. Azure Key Value has been used to securely store sensitive data. More info here: [Create an Azure Key Vault-backed secret scope](https://docs.microsoft.com/en-us/azure/databricks/security/secrets/secret-scopes#--create-an-azure-key-vault-backed-secret-scope)"],"metadata":{}},{"cell_type":"code","source":["val partitionKey = dbutils.widgets.get(\"partitionKey\").toInt\nval prevPartitionKey = partitionKey\n\nval scope = \"key-vault-secrets\"\n\nval storageAccount = \"dmstore2\";\nval storageKey = dbutils.secrets.get(scope, \"dmstore2-2\");\n\nval server = dbutils.secrets.get(scope, \"srv001\").concat(\".database.windows.net\");\nval database = dbutils.secrets.get(scope, \"db001\");\nval user = dbutils.secrets.get(scope, \"dbuser001\");\nval password = dbutils.secrets.get(scope, \"dbpwd001\");\nval table = \"dbo.LINEITEM_LOADTEST\"\n\nval url = s\"jdbc:sqlserver://$server;databaseName=$database;\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">partitionKey: Int = 199810\nprevPartitionKey: Int = 199810\nscope: String = key-vault-secrets\nstorageAccount: String = dmstore2\nstorageKey: String = [REDACTED]\nserver: String = [REDACTED].database.windows.net\ndatabase: String = [REDACTED]\nuser: String = [REDACTED]\npassword: String = [REDACTED]\ntable: String = dbo.LINEITEM_LOADTEST\nurl: String = jdbc:sqlserver://[REDACTED].database.windows.net;databaseName=[REDACTED];\n</div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["Configure Spark to access Azure Blob Store"],"metadata":{}},{"cell_type":"code","source":["spark.conf.set(s\"fs.azure.account.key.$storageAccount.blob.core.windows.net\", storageKey);"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["Load the Parquet file generated in `00-create-parquet-file` notebook that contains LINEITEM data partitioned by Year and Month. Make sure only the specified partion is loaded"],"metadata":{}},{"cell_type":"code","source":["val li = spark\n  .read\n  .parquet(s\"wasbs://tpch@$storageAccount.blob.core.windows.net/10GB/parquet/lineitem\")\n  .filter($\"L_PARTITION_KEY\" === partitionKey)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">li: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [L_ORDERKEY: int, L_PARTKEY: int ... 15 more fields]\n</div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["All columns are shown load as nullable, even if they were originally set to NOT NULL, so we will need to fix this to make sure data can be loaded correctly. \n\nSchema needs to be defined explicitly as connector is very sensitive to nullability, as per the following issue [Nullable column mismatch between Spark DataFrame & SQL Table Error](\nhttps://github.com/microsoft/sql-spark-connector/issues/5), so we need to explicity create the schema and apply it to the loaded data"],"metadata":{}},{"cell_type":"code","source":["import org.apache.spark.sql.types._\n\nval schema = StructType(\n    StructField(\"L_ORDERKEY\", IntegerType, false) ::\n    StructField(\"L_PARTKEY\", IntegerType, false) ::\n    StructField(\"L_SUPPKEY\", IntegerType, false) ::  \n    StructField(\"L_LINENUMBER\", IntegerType, false) ::\n    StructField(\"L_QUANTITY\", DecimalType(15,2), false) ::\n    StructField(\"L_EXTENDEDPRICE\", DecimalType(15,2), false) ::\n    StructField(\"L_DISCOUNT\", DecimalType(15,2), false) ::\n    StructField(\"L_TAX\", DecimalType(15,2), false) ::\n    StructField(\"L_RETURNFLAG\", StringType, false) ::\n    StructField(\"L_LINESTATUS\", StringType, false) ::\n    StructField(\"L_SHIPDATE\", DateType, false) ::\n    StructField(\"L_COMMITDATE\", DateType, false) ::\n    StructField(\"L_RECEIPTDATE\", DateType, false) ::\n    StructField(\"L_SHIPINSTRUCT\", StringType, false) ::  \n    StructField(\"L_SHIPMODE\", StringType, false) ::  \n    StructField(\"L_COMMENT\", StringType, false) ::  \n    StructField(\"L_PARTITION_KEY\", IntegerType, false) ::  \n    Nil)\n    \nval li2 = spark.createDataFrame(li.rdd, schema)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">import org.apache.spark.sql.types._\nschema: org.apache.spark.sql.types.StructType = StructType(StructField(L_ORDERKEY,IntegerType,false), StructField(L_PARTKEY,IntegerType,false), StructField(L_SUPPKEY,IntegerType,false), StructField(L_LINENUMBER,IntegerType,false), StructField(L_QUANTITY,DecimalType(15,2),false), StructField(L_EXTENDEDPRICE,DecimalType(15,2),false), StructField(L_DISCOUNT,DecimalType(15,2),false), StructField(L_TAX,DecimalType(15,2),false), StructField(L_RETURNFLAG,StringType,false), StructField(L_LINESTATUS,StringType,false), StructField(L_SHIPDATE,DateType,false), StructField(L_COMMITDATE,DateType,false), StructField(L_RECEIPTDATE,DateType,false), StructField(L_SHIPINSTRUCT,StringType,false), StructField(L_SHIPMODE,StringType,false), StructField(L_COMMENT,StringType,false), StructField(L_PARTITION_KEY,IntegerType,false))\nli2: org.apache.spark.sql.DataFrame = [L_ORDERKEY: int, L_PARTKEY: int ... 15 more fields]\n</div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["Create the T-SQL script need to extract information on the partition that will be loaded into Azure SQL"],"metadata":{}},{"cell_type":"code","source":["val sqlPartitionValueInfo = \ns\"\"\"\nSELECT\n\t*\nFROM\n(\n\tSELECT\n\t\tprv.[boundary_id] AS partitionId,\n\t\tCAST(prv.[value] AS INT) AS [value],\n\t\tCAST(LAG(prv.[value]) OVER (ORDER BY prv.[boundary_id]) AS INT) AS [prevValue],\n\t\tCAST(LEAD(prv.[value]) OVER (ORDER BY prv.[boundary_id]) AS INT) AS [nextValue]\n\tFROM\n\t\tsys.[indexes] i\n\tINNER JOIN\n\t\tsys.[data_spaces] dp ON i.[data_space_id] = dp.[data_space_id]\n\tINNER JOIN\n\t\tsys.[partition_schemes] ps ON dp.[data_space_id] = ps.[data_space_id]\n\tINNER JOIN\n\t\tsys.[partition_range_values] prv ON [prv].[function_id] = [ps].[function_id]\n\tWHERE\n\t\ti.[object_id] = OBJECT_ID('${table}')\n\tAND\n\t\ti.[index_id] IN (0,1)\n) AS [pi]\nWHERE\n\t[value] = ${partitionKey}\n\"\"\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">sqlPartitionValueInfo: String =\n&quot;\nSELECT\n\t*\nFROM\n(\n\tSELECT\n\t\tprv.[boundary_id] AS partitionId,\n\t\tCAST(prv.[value] AS INT) AS [value],\n\t\tCAST(LAG(prv.[value]) OVER (ORDER BY prv.[boundary_id]) AS INT) AS [prevValue],\n\t\tCAST(LEAD(prv.[value]) OVER (ORDER BY prv.[boundary_id]) AS INT) AS [nextValue]\n\tFROM\n\t\tsys.[indexes] i\n\tINNER JOIN\n\t\tsys.[data_spaces] dp ON i.[data_space_id] = dp.[data_space_id]\n\tINNER JOIN\n\t\tsys.[partition_schemes] ps ON dp.[data_space_id] = ps.[data_space_id]\n\tINNER JOIN\n\t\tsys.[partition_range_values] prv ON [prv].[function_id] = [ps].[function_id]\n\tWHERE\n\t\ti.[object_id] = OBJECT_ID('dbo.LINEITEM_LOADTEST')\n\tAND\n\t\ti.[index_id] IN (0,1)\n) AS [pi]\nWHERE\n\t[value] = 199810\n&quot;\n</div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["Setup JDBC connection, needed to execute ad-hoc T-SQL statement on Azure SQL"],"metadata":{}},{"cell_type":"code","source":["val connectionProperties = new java.util.Properties()\nconnectionProperties.put(\"user\", user)\nconnectionProperties.put(\"password\", password)\nconnectionProperties.setProperty(\"Driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\nval conn = java.sql.DriverManager.getConnection(url, connectionProperties)\nval st = conn.createStatement()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">connectionProperties: java.util.Properties = {user=[REDACTED], password=[REDACTED], Driver=com.microsoft.sqlserver.jdbc.SQLServerDriver}\nconn: java.sql.Connection = ConnectionID:67 ClientConnectionId: 56453dd6-a03e-4fb7-8c90-e584323a3a13\nst: java.sql.Statement = SQLServerStatement:83\n</div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["Load Azure SQL partition metadata"],"metadata":{}},{"cell_type":"code","source":["case class PartitionInfo(partitionId: Int, value: Int, prevValue: Option[Int], nextValue: Option[Int]);\nval piDF = spark.read.jdbc(url, s\"($sqlPartitionValueInfo) AS t\", connectionProperties)\nval pi= piDF.as[PartitionInfo].collect()(0)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">defined class PartitionInfo\npiDF: org.apache.spark.sql.DataFrame = [partitionId: int, value: int ... 2 more fields]\npi: PartitionInfo = PartitionInfo(82,199810,Some(199809),None)\n</div>"]}}],"execution_count":18},{"cell_type":"markdown","source":["Create on Azure SQL a staging table where data will be bulk loaded"],"metadata":{}},{"cell_type":"code","source":["st.execute(s\"DROP TABLE IF EXISTS ${table}_STG_${partitionKey}\")\nst.execute(s\"SELECT TOP (0) * INTO ${table}_STG_${partitionKey} FROM ${table}\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res118: Boolean = false\n</div>"]}}],"execution_count":20},{"cell_type":"markdown","source":["Create the same indexes that the target table has, in order to allow switch-in"],"metadata":{}},{"cell_type":"code","source":["st.execute(s\"CREATE CLUSTERED INDEX IXC ON ${table}_STG_${partitionKey} ([L_COMMITDATE], [L_PARTITION_KEY])\")\nst.execute(s\"CREATE UNIQUE NONCLUSTERED INDEX IX1 ON ${table}_STG_${partitionKey} ([L_ORDERKEY], [L_LINENUMBER], [L_PARTITION_KEY])\")\nst.execute(s\"CREATE NONCLUSTERED INDEX IX2 ON ${table}_STG_${partitionKey} ([L_PARTKEY], [L_PARTITION_KEY])\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res119: Boolean = false\n</div>"]}}],"execution_count":22},{"cell_type":"markdown","source":["Load the staging table"],"metadata":{}},{"cell_type":"code","source":["li2.write \n  .format(\"com.microsoft.sqlserver.jdbc.spark\") \n  .mode(\"overwrite\")   \n  .option(\"truncate\", \"true\") \n  .option(\"url\", url) \n  .option(\"dbtable\", s\"${table}_STG_${partitionKey}\") \n  .option(\"user\", user) \n  .option(\"password\", password) \n  .option(\"reliabilityLevel\", \"BEST_EFFORT\") \n  .option(\"tableLock\", \"false\") \n  .option(\"batchsize\", \"100000\")   \n  .save()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":24},{"cell_type":"markdown","source":["Add a check constraint on the table to allow switch-in"],"metadata":{}},{"cell_type":"code","source":["if (pi.prevValue == None) {\n  st.execute(s\"ALTER TABLE ${table}_STG_${partitionKey} ADD CONSTRAINT ck_partition_${partitionKey} CHECK (L_PARTITION_KEY <= ${pi.value})\")\n} else {\n  st.execute(s\"ALTER TABLE ${table}_STG_${partitionKey} ADD CONSTRAINT ck_partition_${partitionKey} CHECK (L_PARTITION_KEY > ${pi.prevValue.get} AND L_PARTITION_KEY <= ${pi.value})\")\n}"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res121: Boolean = false\n</div>"]}}],"execution_count":26},{"cell_type":"markdown","source":["Delete data in existing partition of target table, execute the switch-in and drop the staging table"],"metadata":{}},{"cell_type":"code","source":["st.execute(s\"TRUNCATE TABLE ${table} WITH (PARTITIONS (${pi.partitionId}))\")\nst.execute(s\"ALTER TABLE ${table}_STG_${partitionKey} SWITCH TO ${table} PARTITION ${pi.partitionId}\")\nst.execute(s\"DROP TABLE ${table}_STG_${partitionKey}\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res122: Boolean = false\n</div>"]}}],"execution_count":28},{"cell_type":"markdown","source":["Done!"],"metadata":{}},{"cell_type":"code","source":["dbutils.notebook.exit(partitionKey.toString)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/plain":["199810"]}}],"execution_count":30}],"metadata":{"name":"03b-parallel-switch-in-load-into-partitioned-table-single","notebookId":964636935775860},"nbformat":4,"nbformat_minor":0}